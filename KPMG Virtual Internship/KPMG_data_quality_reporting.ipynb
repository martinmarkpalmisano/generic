{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f71a00c",
   "metadata": {},
   "source": [
    "#### Getting set up\n",
    "\n",
    "- Move this notebook into dedicated folder - either within a 'Repos' folder if you already have one, or create one on your desktop\n",
    "    - This will the folder to define as your 'workspace'\n",
    "- To get started, you'll need to set up a virtual environment\n",
    "- Click 'Select Kernel' in the top right corner, or press Ctrl+Shift+P and select/search 'Notebook: Select Notebok Kernel' from the menu\n",
    "- Select 'Python Environments' -> 'Create Python Environment' -> 'Venv' -> Then choose a python interpreter (which you hopefully have installed from out last onsite)\n",
    "    - If you don't have python installed, you should be able to get this yourself from Software Centre on your JLR laptop\n",
    "- Uncomment the cell below and execute to install the required packages to connect to Google BigQuery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas google-cloud-bigquery google-cloud-bigquery-storage db-dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: You'll need to install the google cloud SDK from software centre and sign in via the SDK interface; run 'gcloud init'\n",
    "# TODO: Once signed in, run 'gcloud auth application-default login' from SDK interface\n",
    "# TODO: When ready, install the required packages from the cell above (uncomment first!)\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# TODO: Write a query to select the data from the 'sellers_corrupted' table within the 'analytics_onsite' dataset from the 'jlr-dl-cat-training' environment\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM `jlr-dl-cat-training.analytics_onsite.sellers_corrupted` \n",
    "\"\"\"\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(project=\"jlr-dl-cat-training\") # TODO:Enter the project which will be billed here\n",
    "\n",
    "# Run the query and load into a DataFrame\n",
    "df = client.query(query).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a488e2e4",
   "metadata": {},
   "source": [
    "2. Display basic info: shape, column names, data types. Preview the first 10 row of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4230f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Report on the shape of the data, column names, data types.\n",
    "df.shape\n",
    "df.info()\n",
    "\n",
    "#TODO: Preview the first 10 row of data.\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485f0b18",
   "metadata": {},
   "source": [
    "#### Exercise 1: Standardizing Text Case\n",
    "\n",
    "**Goal:** Normalize text:\n",
    "- `seller_city` → lowercase and whitespaces removed\n",
    "- `seller_state` → uppercase and whitespaces removed\n",
    "- Assign to new columns ('seller_city_clean', 'seller_state_clean') to preserve original data\n",
    "\n",
    "**Hint:**\n",
    "- String functions can be applied using `.str.____`\n",
    "- Functions can be strung together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a79f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize city and state names\n",
    "#TODO: Write code here, reference new columns 'seller_city_clean', 'seller_state_clean'\n",
    "df['seller_city_clean'] = df['seller_city'].str.lower().str.replace(' ', '')\n",
    "df['seller_state_clean'] = df['seller_state'].str.upper().str.replace(' ', '')\n",
    "\n",
    "# Preview changes\n",
    "filtered_df = df[df['seller_city'].str.contains(r'[A-Z]', na=False)]\n",
    "filtered_df[['seller_city', 'seller_city_clean', 'seller_state', 'seller_state_clean']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d3f8b",
   "metadata": {},
   "source": [
    "#### Exercise 2: Identifying Duplicates\n",
    "\n",
    "**Goal:** Count duplicate data entries, then remove them\n",
    "\n",
    "**Hint:**\n",
    "- Use `.sum()` to count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd2950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Count duplicate entries\n",
    "df.duplicated().sum()\n",
    "\n",
    "#TODO: Remove duplicate entries\n",
    "df1 = df.drop_duplicates()\n",
    "df1.duplicated().sum()\n",
    "\n",
    "#TODO: Verify any duplicate entries have been removed\n",
    "df.info()\n",
    "df1.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a3cb8",
   "metadata": {},
   "source": [
    "### Exercise 3: Counting Unique Values per Column\n",
    "\n",
    "**Goal:** Determine how many unique values exist in each column of the dataset.\n",
    "\n",
    "**Instructions:**\n",
    "- Use the `.nunique()` method to count unique values.\n",
    "- Display the result of each column in a summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216debd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Count unique entries for each column\n",
    "df1.nunique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603b75bb",
   "metadata": {},
   "source": [
    "#### Exercise 4: Detecting Whitespace Issues\n",
    "\n",
    "**Goal:** Identify rows in `seller_city` and `seller_state` with:\n",
    "- Leading/trailing whitespace (these were removed in exercise 1, but this time we want to detect them first, not just blindly removed them)\n",
    "- Multiple consecutive spaces\n",
    "- Cells that are only whitespace\n",
    "\n",
    "**Hint:**\n",
    "- You will need to evaluate whether the value meets certain conditions\n",
    "- Use `'  '` (double space) to check for multiple consecutive spaces\n",
    "- Check whether `.str.strip()` also results in a blank cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5201e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect whitespace issues in string columns\n",
    "def detect_whitespace_issues(series):\n",
    "    return series.apply(lambda x: isinstance(x, str) and (\n",
    "        #TODO: insert logic here to identify problematic values\n",
    "        x != x.strip() or '  ' in x or x.strip() == \"\"\n",
    "        )        )\n",
    "\n",
    "# Apply to relevant columns\n",
    "for col in ['seller_city', 'seller_state']:\n",
    "    issues = detect_whitespace_issues(df[col])\n",
    "    if issues.any():\n",
    "        print(f\"Whitespace issues in column '{col}':\")\n",
    "        display(df.loc[issues, [col]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32d4fba",
   "metadata": {},
   "source": [
    "#### Exploring dataframes in python\n",
    "\n",
    "It's often difficult to understand how dataframes look using pure python, but a little easier using Notebooks.\n",
    "<br> However, there are tools in VS Code to help you interrogate data tables!\n",
    "\n",
    "**Goal:** Install the 'Data Wrangler' Extension and explore the dataframe; sort, filter or clean data.\n",
    "- Use the UI to understand how to turn your interactions into python code\n",
    "\n",
    "**Hint:**\n",
    "- Data wrangler is not a python package, it's a VS Code extension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cac6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Install the 'Data Wrangler' Extension. You don't need to write any code here.\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e2c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "def clean_data(df):\n",
    "    # Convert text to uppercase in column: 'seller_city'\n",
    "    df['seller_city'] = df['seller_city'].str.upper()\n",
    "    # Convert text to lowercase in column: 'seller_state'\n",
    "    df['seller_state'] = df['seller_state'].str.lower()\n",
    "    return df\n",
    "\n",
    "df_clean = clean_data(df.copy())\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b4ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "def clean_data(df):\n",
    "    # Drop duplicate rows across all columns\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "df_clean = clean_data(df.copy())\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd8d3e",
   "metadata": {},
   "source": [
    "#### Automating data exploration\n",
    "\n",
    "- Understanding data before diving into analysis is often skipped, a crucial step when working with a dataset for the first time.\n",
    "    - Without a clear grasp of the structure, quality, and context of your data, you risk drawing misleading conclusions or missing key insights altogether.\n",
    "<br> \n",
    "- This isn’t a one-time skill - it’s a recurring discipline. Every new dataset brings its own quirks, inconsistencies, and hidden patterns.\n",
    "    - Whether it’s missing values, unexpected formats, or outliers, each dataset demands a fresh round of exploration and validation.\n",
    "<br> \n",
    "- Automating parts of this initial exploration can dramatically speed up the process and reduce errors from misunderstandings.\n",
    "    - It frees up time for deeper, more strategic thinking and ensures a consistent, repeatable approach to data understanding across projects.\n",
    "<br> \n",
    "\n",
    "**Goal:** Use `pandas-dq` and `ydata-profiling` to understand the dataframe\n",
    "- Install the `pandas-dq` and `ydata-profiling`\n",
    "- Run the cells below\n",
    "\n",
    "**Hint:**\n",
    "- You can install packages directly from the \n",
    "- Change the verbose setting between 0 and 1 for `pandas-dq`\n",
    "- `ydata-profiling` you will likely encounter errors relating to package dependencies and versions, resolve the dependencies to install successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a056c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: install pandas-dq\n",
    "\n",
    "# %pip install pandas-dq\n",
    "# TODO: install ydata-profiling\n",
    "%pip install ydata_profiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2a74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_dq as dq\n",
    "\n",
    "dq_report = dq.dq_report(df1, target= None, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a88263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "profile = ProfileReport(df, title=\"Data Quality Report\", explorative=True)\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30be324",
   "metadata": {},
   "source": [
    "#### Advanced challenge:\n",
    "\n",
    "Highly cardinal columns (those with a large number of unique values) can pose significant challenges in data analysis and modelling.\n",
    "There are several non-modeling reasons to resolve high-cardinality columns into groups:\n",
    "- **Clarity**: Charts and summaries become cluttered and unreadable with too many categories\n",
    "- **Insight Discovery**: Grouping helps highlight broader trends and patterns that might be obscured by granular detail\n",
    "- **Comparability**: It’s easier to compare groups than hundreds of individual categories\n",
    "- **Audience Understanding**: Humans struggle to draw conclusions from highly fragmented data due to cognitive overload\n",
    "- **Storytelling**: Grouped data supports clearer narratives and more compelling insights\n",
    "- **Error Detection**: High cardinality can mask typos, inconsistent naming, or duplicate entries (e.g., \"NYC\", \"New York\", \"NewYork\")\n",
    "- **Standardization**: Grouping helps enforce consistency across datasets, especially when merging or joining data from multiple sources\n",
    "\n",
    "\n",
    "\n",
    "**Task:**\n",
    "The text is the 'seller_city' column has high cardinality (lots of unique values), due a mixture of formatting, spelling errors and different naming conventions created by different users.\n",
    "\n",
    "Develop a method to cluster/categorise/reassign the seller city into a smaller number of cities.\n",
    "\n",
    "**Hint:**\n",
    "- Use information from other columns to aid your reclassification\n",
    "- You're not expected to know how to do this from memory! Use co-pilot to translate your strategy into code.\n",
    "\n",
    "For examples, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2653ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de1dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import fuzz #TODO: Run the cell above to install rapidfuzz first\n",
    "\n",
    "# Convert column to list of strings\n",
    "values = df[\"seller_city\"].astype(str).tolist()\n",
    "distinct_values = list(dict.fromkeys(values))\n",
    "n = len(distinct_values)\n",
    "\n",
    "# Initialize similarity matrix\n",
    "similarity_matrix = np.zeros((n, n))\n",
    "\n",
    "# Compute pairwise similarity scores\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        similarity_matrix[i, j] = fuzz.ratio(distinct_values[i], distinct_values[j])\n",
    "\n",
    "# Create a DataFrame for the similarity matrix\n",
    "sim_df = pd.DataFrame(similarity_matrix, index=distinct_values, columns=distinct_values)\n",
    "\n",
    "# Extract top N most similar city name pairs (excluding self-comparisons)\n",
    "top_n = 20\n",
    "similarities = []\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        similarities.append((distinct_values[i], distinct_values[j], similarity_matrix[i, j]))\n",
    "\n",
    "# Sort by similarity score in descending order\n",
    "similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Display top N most similar pairs\n",
    "for city1, city2, score in similarities[:top_n]:\n",
    "    print(f\"{city1} <-> {city2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
